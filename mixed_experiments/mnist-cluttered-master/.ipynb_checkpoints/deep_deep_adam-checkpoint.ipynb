{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/theano/tensor/signal/downsample.py:6: UserWarning: downsample module has been moved to the theano.tensor.signal.pool module.\n",
      "  \"downsample module has been moved to the theano.tensor.signal.pool module.\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import Image\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "import lasagne\n",
    "\n",
    "from collections import OrderedDict\n",
    "from lasagne import utils\n",
    "\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_seed(n):\n",
    "    np.random.seed(n)\n",
    "    lasagne.random.set_rng(np.random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_mlp(input_var=None, BN=False):\n",
    "    l_in = lasagne.layers.InputLayer(shape=(None, 1, 28, 28),\n",
    "                                     input_var=input_var)\n",
    "    if BN:\n",
    "        l_in = lasagne.layers.batch_norm(l_in)\n",
    "\n",
    "################## 1st HIDDEN LAYER ############################\n",
    "\n",
    "    l_hid1 = lasagne.layers.DenseLayer(\n",
    "            l_in, num_units=100,\n",
    "            nonlinearity=lasagne.nonlinearities.rectify,\n",
    "            W=lasagne.init.GlorotUniform())\n",
    "    if BN:\n",
    "        l_hid1 = lasagne.layers.batch_norm(l_hid1)\n",
    "\n",
    "################## 2nd HIDDEN LAYER ############################\n",
    "\n",
    "    l_hid2 = lasagne.layers.DenseLayer(\n",
    "            l_hid1, num_units=100,\n",
    "            nonlinearity=lasagne.nonlinearities.rectify)\n",
    "    if BN:\n",
    "        l_hid2 = lasagne.layers.batch_norm(l_hid2)\n",
    "\n",
    "################## 3rd HIDDEN LAYER ############################\n",
    "\n",
    "    l_hid3 = lasagne.layers.DenseLayer(\n",
    "            l_hid2, num_units=100,\n",
    "            nonlinearity=lasagne.nonlinearities.rectify)\n",
    "    if BN:\n",
    "        l_hid3 = lasagne.layers.batch_norm(l_hid3)\n",
    "\n",
    "################## 4th HIDDEN LAYER ############################\n",
    "\n",
    "    l_hid4 = lasagne.layers.DenseLayer(\n",
    "            l_hid3, num_units=100,\n",
    "            nonlinearity=lasagne.nonlinearities.rectify)\n",
    "    if BN:\n",
    "        l_hid4 = lasagne.layers.batch_norm(l_hid4)\n",
    "\n",
    "################## 5th HIDDEN LAYER ############################\n",
    "\n",
    "    l_hid5 = lasagne.layers.DenseLayer(\n",
    "            l_hid4, num_units=100,\n",
    "            nonlinearity=lasagne.nonlinearities.rectify)\n",
    "    if BN:\n",
    "        l_hid5 = lasagne.layers.batch_norm(l_hid5)\n",
    "\n",
    "################## 6th HIDDEN LAYER ############################\n",
    "\n",
    "    l_hid6 = lasagne.layers.DenseLayer(\n",
    "            l_hid5, num_units=100,\n",
    "            nonlinearity=lasagne.nonlinearities.rectify)\n",
    "    if BN:\n",
    "        l_hid6 = lasagne.layers.batch_norm(l_hid6)\n",
    "\n",
    "################## 7th HIDDEN LAYER ############################\n",
    "\n",
    "    l_hid7 = lasagne.layers.DenseLayer(\n",
    "        l_hid6, num_units=100,\n",
    "        nonlinearity=lasagne.nonlinearities.rectify)\n",
    "    if BN:\n",
    "        l_hid7 = lasagne.layers.batch_norm(l_hid7)\n",
    "\n",
    "################## 8th HIDDEN LAYER ############################\n",
    "\n",
    "    l_hid8 = lasagne.layers.DenseLayer(\n",
    "        l_hid7, num_units=100,\n",
    "        nonlinearity=lasagne.nonlinearities.rectify)\n",
    "    if BN:\n",
    "        l_hid8= lasagne.layers.batch_norm(l_hid8)\n",
    "\n",
    "################## 9th HIDDEN LAYER ############################\n",
    "\n",
    "    l_hid9 = lasagne.layers.DenseLayer(\n",
    "        l_hid8, num_units=100,\n",
    "        nonlinearity=lasagne.nonlinearities.rectify)\n",
    "    if BN:\n",
    "        l_hid9 = lasagne.layers.batch_norm(l_hid9)\n",
    "\n",
    "################## 10th HIDDEN LAYER ############################\n",
    "\n",
    "    l_hid10 = lasagne.layers.DenseLayer(\n",
    "        l_hid9, num_units=100,\n",
    "        nonlinearity=lasagne.nonlinearities.rectify)\n",
    "    if BN:\n",
    "        l_hid10 = lasagne.layers.batch_norm(l_hid10)\n",
    "\n",
    "################## OUT LAYER ############################\n",
    "\n",
    "    l_out = lasagne.layers.DenseLayer(\n",
    "            l_hid10, num_units=10,\n",
    "            nonlinearity=lasagne.nonlinearities.softmax)\n",
    "    return l_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    subprocess.call(['th', '-l', 'my_example'])\n",
    "    n, d = 10000, 3600\n",
    "    train_images = np.zeros((n, d), dtype=np.uint8)\n",
    "    train_labels = np.zeros(n, dtype=np.uint8)\n",
    "    for i in range(n):\n",
    "        image_open = Image.open(\"clMNIST/example\" + str(i) + \".png\")\n",
    "        a = np.array(image_open.getdata())\n",
    "        train_images[i] = a \n",
    "        train_labels[i] = np.uint(np.loadtxt(\"clMNIST/y\" + str(i)))\n",
    "    return np.reshape(train_images, (-1, 1, 60, 60)), np.ravel(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.arange(len(inputs))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt], targets[excerpt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def adam_update(loss_or_grads, params, learning_rate=1e-3, beta1=0.9,\n",
    "                        beta2=0.999, epsilon=1e-8):\n",
    "    all_grads = lasagne.updates.get_or_compute_grads(loss_or_grads, params)\n",
    "    t_prev = theano.shared(utils.floatX(0.))\n",
    "    updates = OrderedDict()\n",
    "\n",
    "    t = t_prev + 1\n",
    "    a_t = learning_rate * T.sqrt(1 - beta2 ** t) / (1 - beta1 ** t)\n",
    "\n",
    "    for param, g_t in zip(params, all_grads):\n",
    "        value = param.get_value(borrow=True)\n",
    "        m_prev = theano.shared(np.zeros(value.shape, dtype=value.dtype),\n",
    "                               broadcastable=param.broadcastable)\n",
    "        v_prev = theano.shared(np.zeros(value.shape, dtype=value.dtype),\n",
    "                               broadcastable=param.broadcastable)\n",
    "\n",
    "        m_t = beta1 * m_prev + (1 - beta1) * g_t\n",
    "        v_t = beta2 * v_prev + (1 - beta2) * g_t ** 2\n",
    "        step = a_t * m_t / (T.sqrt(v_t) + epsilon)\n",
    "\n",
    "        updates[m_prev] = m_t\n",
    "        updates[v_prev] = v_t\n",
    "        updates[param] = param - step\n",
    "\n",
    "    updates[t_prev] = t\n",
    "    return updates\n",
    "\n",
    "\n",
    "def adam_update2(loss_or_grads, params, learning_rate=1e-3, beta1=0.9,\n",
    "                 beta2=0.999, epsilon=1e-6):\n",
    "    all_grads = lasagne.updates.get_or_compute_grads(loss_or_grads, params)\n",
    "\n",
    "\n",
    "    t_prev = theano.shared(utils.floatX(0.))\n",
    "    updates = OrderedDict()\n",
    "\n",
    "    t = t_prev + 1\n",
    "    a_t = learning_rate * T.sqrt(1 - beta2 ** t) / (1 - beta1 ** t)\n",
    "\n",
    "    for param, g_t in zip(params, all_grads):\n",
    "        value = param.get_value(borrow=True)\n",
    "        m_prev = theano.shared(np.zeros(value.shape, dtype=value.dtype),\n",
    "                               broadcastable=param.broadcastable)\n",
    "        v_prev = theano.shared(np.zeros(value.shape, dtype=value.dtype),\n",
    "                               broadcastable=param.broadcastable)\n",
    "\n",
    "        m_t = beta1 * m_prev + (1 - beta1) * g_t\n",
    "        v_t = beta2 * v_prev + (1 - beta2) * g_t ** 2\n",
    "        step = a_t * m_t / (T.sqrt(v_t + epsilon))\n",
    "\n",
    "        updates[m_prev] = m_t\n",
    "        updates[v_prev] = v_t\n",
    "        updates[param] = param - step\n",
    "\n",
    "    updates[t_prev] = t\n",
    "    return updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sz = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_method(method, model='mlp', BN=False, num_epochs=50, alpha=0.1, mu=0.9, beta1=0.9, beta2=0.999, echo=False, \n",
    "               batch_size=100, epsilon=1e-8):\n",
    "    input_var = T.tensor4('inputs')\n",
    "    target_var = T.ivector('targets')\n",
    "\n",
    "    if echo:\n",
    "        print(\"Building model and compiling functions...\")\n",
    "    if model == 'mlp':\n",
    "        network = build_mlp(input_var, BN)\n",
    "    else:\n",
    "        print(\"Unrecognized model type %r.\" % model)\n",
    "        return\n",
    "\n",
    "    prediction = lasagne.layers.get_output(network)\n",
    "    loss = lasagne.objectives.categorical_crossentropy(prediction, target_var)\n",
    "    loss = loss.mean()\n",
    "    train_acc = T.mean(T.eq(T.argmax(prediction, axis=1), target_var),\n",
    "                 dtype=theano.config.floatX)\n",
    "\n",
    "    params = lasagne.layers.get_all_params(network, trainable=True)\n",
    "\n",
    "    if method == lasagne.updates.sgd:\n",
    "        updates = method(loss, params, learning_rate=alpha)\n",
    "    elif method == lasagne.updates.momentum:\n",
    "        updates = method(loss, params, learning_rate=alpha, momentum=mu)\n",
    "    elif method == lasagne.updates.adam:\n",
    "        updates = method(loss, params, learning_rate=alpha, beta1=beta1)\n",
    "    elif method == adam_update or method == adam_update2:\n",
    "        updates = method(loss, params, learning_rate=alpha, beta1=beta1, beta2=beta2, epsilon=epsilon)\n",
    "    else:\n",
    "        updates = method(loss, params, learning_rate=alpha, epsilon=epsilon)\n",
    "\n",
    "\n",
    "    test_prediction = lasagne.layers.get_output(network, deterministic=True)\n",
    "    test_loss = lasagne.objectives.categorical_crossentropy(test_prediction,\n",
    "                                                            target_var)\n",
    "    test_loss = test_loss.mean()\n",
    "    test_acc = T.mean(T.eq(T.argmax(test_prediction, axis=1), target_var),\n",
    "                      dtype=theano.config.floatX)\n",
    "\n",
    "    train_fn = theano.function([input_var, target_var], loss, updates=updates)\n",
    "    train_fn_acc = theano.function([input_var, target_var], train_acc)\n",
    "    val_fn = theano.function([input_var, target_var], [test_loss, test_acc])\n",
    "\n",
    "    if echo:\n",
    "        print(\"Starting training...\")\n",
    "\n",
    "    res = dict()\n",
    "    arr_train_err = []\n",
    "    arr_val_err = []\n",
    "    arr_train_acc = []\n",
    "    arr_val_acc = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_err = 0\n",
    "        train_batches = 0\n",
    "        train_acc = 0\n",
    "        start_time = time.time()\n",
    "        \n",
    "        train_inputs, train_targets = get_data()\n",
    "        \n",
    "        for batch in iterate_minibatches(train_inputs, train_targets, batch_size, shuffle=True):\n",
    "            inputs, targets = batch\n",
    "            err = train_fn(inputs, targets)\n",
    "            acc = train_fn_acc(inputs, targets)\n",
    "            train_err += err\n",
    "            train_acc += acc\n",
    "            train_batches += 1\n",
    "\n",
    "        arr_train_err.append(train_err / train_batches)\n",
    "        arr_train_acc.append(train_acc / train_batches * 100)\n",
    "        \n",
    "        val_inputs, val_targets = get_data()\n",
    "\n",
    "        val_err = 0\n",
    "        val_acc = 0\n",
    "        val_batches = 0\n",
    "        for batch in iterate_minibatches(val_inputs, val_targets, batch_size, shuffle=False):\n",
    "            inputs, targets = batch\n",
    "            err, acc = val_fn(inputs, targets)\n",
    "            val_err += err\n",
    "            val_acc += acc\n",
    "            val_batches += 1\n",
    "\n",
    "        arr_val_err.append(val_err / val_batches)\n",
    "        arr_val_acc.append(val_acc / val_batches * 100)\n",
    "\n",
    "        if echo:\n",
    "            print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "                epoch + 1, num_epochs, time.time() - start_time))\n",
    "            print(\"  training loss:\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "#             print(\"  validation loss:\\t\\t{:.6f}\".format(val_err / val_batches))\n",
    "#             print(\"  validation accuracy:\\t\\t{:.2f} %\".format(\n",
    "#                 val_acc / val_batches * 100))\n",
    "\n",
    "    test_err = 0\n",
    "    test_acc = 0\n",
    "    test_batches = 0\n",
    "    \n",
    "    test_inputs, test_targets = get_data()\n",
    "        \n",
    "    for batch in iterate_minibatches(test_inputs, test_targets, batch_size, shuffle=False):\n",
    "        inputs, targets = batch\n",
    "        err, acc = val_fn(inputs, targets)\n",
    "        test_err += err\n",
    "        test_acc += acc\n",
    "        test_batches += 1\n",
    "\n",
    "    if echo:\n",
    "        print(\"Final results:\")\n",
    "        print(\"  test loss:\\t\\t\\t{:.6f}\".format(test_err / test_batches))\n",
    "        print(\"  test accuracy:\\t\\t{:.2f} %\".format(\n",
    "            test_acc / test_batches * 100))\n",
    "\n",
    "    res['train_err'] = np.array(arr_train_err)\n",
    "    res['val_err'] = np.array(arr_val_err)\n",
    "    res['train_acc'] = np.array(arr_train_acc)\n",
    "    res['val_acc'] = np.array(arr_val_acc)\n",
    "    res['test_err'] = test_err / test_batches\n",
    "    res['test_acc'] = test_acc / test_batches * 100\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Shape mismatch: x has 3600 cols (and 100 rows) but y has 784 rows (and 100 cols)\nApply node that caused the error: Dot22(Reshape{2}.0, W)\nToposort index: 32\nInputs types: [TensorType(float64, matrix), TensorType(float64, matrix)]\nInputs shapes: [(100, 3600), (784, 100)]\nInputs strides: [(28800, 8), (800, 8)]\nInputs values: ['not shown', 'not shown']\nOutputs clients: [[Elemwise{add,no_inplace}(Dot22.0, InplaceDimShuffle{x,0}.0), Elemwise{Composite{(i0 * (Abs(i1) + i2 + i3))}}[(0, 2)](TensorConstant{(1, 1) of 0.5}, Elemwise{add,no_inplace}.0, Dot22.0, InplaceDimShuffle{x,0}.0)]]\n\nHINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-d8d7df69ec9f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mu'time'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mu''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mu'make_seed(100)\\nadam = run_method(adam_update, num_epochs=50, alpha=1e-3, beta2=0.999)'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[1;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[0;32m   2118\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2119\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2120\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2121\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/IPython/core/magics/execution.pyc\u001b[0m in \u001b[0;36mtime\u001b[1;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/IPython/core/magic.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(f, *a, **k)\u001b[0m\n\u001b[0;32m    191\u001b[0m     \u001b[1;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 193\u001b[1;33m         \u001b[0mcall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/IPython/core/magics/execution.pyc\u001b[0m in \u001b[0;36mtime\u001b[1;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[0;32m   1175\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1176\u001b[0m             \u001b[0mst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1177\u001b[1;33m             \u001b[1;32mexec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1178\u001b[0m             \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1179\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-cb85f97ddf07>\u001b[0m in \u001b[0;36mrun_method\u001b[1;34m(method, model, BN, num_epochs, alpha, mu, beta1, beta2, echo, batch_size, epsilon)\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miterate_minibatches\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_targets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m             \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m             \u001b[0merr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m             \u001b[0macc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_fn_acc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m             \u001b[0mtrain_err\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    869\u001b[0m                     \u001b[0mnode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mposition_of_error\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    870\u001b[0m                     \u001b[0mthunk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mthunk\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 871\u001b[1;33m                     storage_map=getattr(self.fn, 'storage_map', None))\n\u001b[0m\u001b[0;32m    872\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    873\u001b[0m                 \u001b[1;31m# old-style linkers raise their own exceptions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/theano/gof/link.pyc\u001b[0m in \u001b[0;36mraise_with_op\u001b[1;34m(node, thunk, exc_info, storage_map)\u001b[0m\n\u001b[0;32m    312\u001b[0m         \u001b[1;31m# extra long error message in that case.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 314\u001b[1;33m     \u001b[0mreraise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc_trace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    315\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    857\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    858\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 859\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    860\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'position_of_error'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Shape mismatch: x has 3600 cols (and 100 rows) but y has 784 rows (and 100 cols)\nApply node that caused the error: Dot22(Reshape{2}.0, W)\nToposort index: 32\nInputs types: [TensorType(float64, matrix), TensorType(float64, matrix)]\nInputs shapes: [(100, 3600), (784, 100)]\nInputs strides: [(28800, 8), (800, 8)]\nInputs values: ['not shown', 'not shown']\nOutputs clients: [[Elemwise{add,no_inplace}(Dot22.0, InplaceDimShuffle{x,0}.0), Elemwise{Composite{(i0 * (Abs(i1) + i2 + i3))}}[(0, 2)](TensorConstant{(1, 1) of 0.5}, Elemwise{add,no_inplace}.0, Dot22.0, InplaceDimShuffle{x,0}.0)]]\n\nHINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node."
     ]
    }
   ],
   "source": [
    "%%time\n",
    "make_seed(100)\n",
    "adam = run_method(adam_update, num_epochs=50, alpha=1e-3, beta2=0.999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "make_seed(100)\n",
    "bn_adam = run_method(adam_update, num_epochs=50, alpha=1e-3, beta2=0.999, BN=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
