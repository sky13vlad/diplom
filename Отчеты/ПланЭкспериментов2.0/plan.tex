\documentclass[12pt,oneside]{article}

\usepackage[russian]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{multicol}
\usepackage[14pt]{extsizes}
\usepackage[linesnumbered,boxed]{algorithm2e}
\usepackage{amsthm}
\usepackage{listings}
\usepackage{epstopdf}


\binoppenalty = 10000
\relpenalty = 10000
\textheight = 23cm
\textwidth = 17cm
\oddsidemargin = 0pt
\topmargin = -1.5cm
\parskip = 0pt
\tolerance = 2000
\flushbottom

\pagestyle{myheadings}

\begin{document}

Исследуем, как влияет батч-нормализация (БН) на различные модификации метода стохастического градиентного спуска (СГД) при обучении нейронных сетей.\\

Предположения:
\begin{itemize}
\item БН улучшает все методы -- то есть ускоряет их сходимость
\item Чем метод лучше работает сам по себе, тем БН слабее ускоряет его сходимость
\item БН улучшает методы сильнее на глубоких сетях
\end{itemize}

\section{Влияние БН на все методы}

Проверим, улучшает ли БН все методы и если улучшает, то оценим насколько. Для этого выполним следующие задачи:

\begin{enumerate}
\item Выберем модификации СГД: СГДм (СГД с моментумом), Адам (адаптивный моментум), Адаград, Ададельта, RMSprop (root mean squared gradients)
\item Выберем датасеты: MNIST, CIFAR-10, cluttered MNIST(?)
\item Выберем архитектуры сети: MLP (multilayer perceptron, 3 полносвязных скрытых слоя по 100 нейронов), CNN (convolutional neural network, 2 сверточных слоя (32 фильтра 5x5 + макс-пулинг с окном 2x2) + 1 скрытый полносвязный с 256 нейронами). 
\item Для каждого метода подберем примерный рейт на всех комбинациях датасетов, архитектур и использования БН
\item Запустим все методы на всех архитектурах и датасетах в комбинации с БН. Сохраним для всех запусков качество на тренировочной и валидационной выборках по эпохам (итерациям)
\item Составим большую таблицу тестового качества со всеми комбинациями датасетов, архитектур, методов и БН
\item Составим большую таблицу относительного улучшения качества всех методов
\item Нарисуем графики для визуализации работы БН (какие графики, как изобразить?)
\end{enumerate}

Возникли следующие проблемы:

\begin{enumerate}
\item Для выбранной CNN БН методы переобучаются на CIFAR-10 при повышении рейта. Возможные пути решения: 
\begin{itemize}
\item брать рейт поменьше
\item добавить dropout
\item уменьшить количество нейронов
\item выбрать датасет посложнее (на MNIST не тестировали, скорее всего, там тоже переобучается)
\end{itemize}
\end{enumerate}

\section{Влияние глубины сети на БН}

Проверим, как улучшает БН методы на глубоких сетях. Для этого выполним следующие задачи:

\begin{enumerate}
\item Выберем те же методы, что и в предыдущем эксперименте (СГД, СГДм, Адам, Адаград, Ададельта, RMSprop)
\item Возьмем датасеты из предыдущего эксперимента: MNIST, CIFAR-10, cluttered MNIST(?)
\item Выберем архитектуры для глубокой сети: (?)
\item Подберем рейт для всех методов на всех комбинациях архитектур, датасетов и БН
\item Запустим все методы на всех архитектурах и датасетах в комбинации с БН. Сохраним для всех запусков качество на тренировочной и валидационной выборках по эпохам (итерациям)
\item Составим большую таблицу тестового качества со всеми комбинациями датасетов, архитектур, методов и БН
\item Составим большую таблицу относительного улучшения качества всех методов
\item Сравним результаты с результатами из предыдущего эксперимента
\end{enumerate}


\section{Влияние БН на методы, использующие моментум}

Гипотеза: БН и моментум уменьшают дисперсию градиента, поэтому их сочетание не приводит к дополнительному улучшению. Для проверки проведем следующие эксперименты:

\begin{enumerate}
\item Отберем методы, использующие моментум: СГДм, Адам, Ададельта (в некоторой степени)
\item Все последующие эксперименты нужно проводить, если из предыдущих результатов мы получили, что на этих методах БН дает меньшую прибавку, чем на остальных (?)
\item Запустим на MLP (3 слоя по 100 нейронов) методы СГД, СГДм с БН и без БН -- сохраним отклонения стохастических градиентов от полных на выбранных итерациях по 50 запускам, а затем усредним их по запускам
\item Нарисуем график для полученных дисперсий 4-х методов
\end{enumerate}

Возникли следующие проблемы:
\begin{enumerate}
\item Получили, что БН увеличивает дисперсию, а моментум уменьшает. Возможные пути решения:
\begin{itemize}
\item Отказаться от эксперимента, так как БН меняет архитектуру сети (оптимизируемую функцию) (число параметров в сети с БН много больше)
\item Полный градиент при БН считать не сразу по всей выборке, а суммировать по батчам (так как при проходе по всей выборке точнее оцениваются средние и дисперсии, то возможно из-за этого стохастический градиент по одному батчу так сильно отклоняется от полного градиента)
\end{itemize}
\end{enumerate}


\section{Прыжки БН Адама}

Гипотеза: на простых датасетах БН Адам имеет на графиках непредсказуемые скачки, которые исчезают при более тщательном подборе параметров метода Адам. Для проверки и подтверждения выполним следующие задачи:

\begin{enumerate}
\item Выберем простой датасет: MNIST
\item Выберем архитектуры сети: MLP, CNN (неглубокие)
\item Добавим в исходники сохранение $v_t$ по итерациям (скользящее среднее квадратов градиентов)
\item Запустим БН Адам несколько раз при стандартных параметрах -- убедимся в наличии скачков, посмотрим, как себя ведут $v_t$ в эти моменты
\item Запустим Адам при стандартных параметрах
\item Запустим БН Адам несколько раз при $\epsilon = 10^{-4}$ и $\epsilon = 10^{-2}$ -- убедимся, что скачки стали меньше, и что $v_t$ стали больше
\item Проверим, как влияет на качество увеличение $\epsilon$ и сравним все со стандартным методом Адам
\end{enumerate}

Вопросы:

\begin{enumerate}
\item Нужно показать, что на более сложных датасетах БН Адам не прыгает?
\end{enumerate}

\end{document}
