{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import Image\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "import lasagne\n",
    "\n",
    "from collections import OrderedDict\n",
    "from lasagne import utils\n",
    "\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_seed(n):\n",
    "    np.random.seed(n)\n",
    "    lasagne.random.set_rng(np.random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def adam_update(loss_or_grads, params, learning_rate=1e-3, beta1=0.9,\n",
    "                        beta2=0.999, epsilon=1e-8):\n",
    "    all_grads = lasagne.updates.get_or_compute_grads(loss_or_grads, params)\n",
    "    t_prev = theano.shared(utils.floatX(0.))\n",
    "    updates = OrderedDict()\n",
    "\n",
    "    t = t_prev + 1\n",
    "    a_t = learning_rate * T.sqrt(1 - beta2 ** t) / (1 - beta1 ** t)\n",
    "\n",
    "    for param, g_t in zip(params, all_grads):\n",
    "        value = param.get_value(borrow=True)\n",
    "        m_prev = theano.shared(np.zeros(value.shape, dtype=value.dtype),\n",
    "                               broadcastable=param.broadcastable)\n",
    "        v_prev = theano.shared(np.zeros(value.shape, dtype=value.dtype),\n",
    "                               broadcastable=param.broadcastable)\n",
    "\n",
    "        m_t = beta1 * m_prev + (1 - beta1) * g_t\n",
    "        v_t = beta2 * v_prev + (1 - beta2) * g_t ** 2\n",
    "        step = a_t * m_t / (T.sqrt(v_t) + epsilon)\n",
    "\n",
    "        updates[m_prev] = m_t\n",
    "        updates[v_prev] = v_t\n",
    "        updates[param] = param - step\n",
    "\n",
    "    updates[t_prev] = t\n",
    "    return updates\n",
    "\n",
    "\n",
    "def adam_update2(loss_or_grads, params, learning_rate=1e-3, beta1=0.9,\n",
    "                 beta2=0.999, epsilon=1e-6):\n",
    "    all_grads = lasagne.updates.get_or_compute_grads(loss_or_grads, params)\n",
    "\n",
    "\n",
    "    t_prev = theano.shared(utils.floatX(0.))\n",
    "    updates = OrderedDict()\n",
    "\n",
    "    t = t_prev + 1\n",
    "    a_t = learning_rate * T.sqrt(1 - beta2 ** t) / (1 - beta1 ** t)\n",
    "\n",
    "    for param, g_t in zip(params, all_grads):\n",
    "        value = param.get_value(borrow=True)\n",
    "        m_prev = theano.shared(np.zeros(value.shape, dtype=value.dtype),\n",
    "                               broadcastable=param.broadcastable)\n",
    "        v_prev = theano.shared(np.zeros(value.shape, dtype=value.dtype),\n",
    "                               broadcastable=param.broadcastable)\n",
    "\n",
    "        m_t = beta1 * m_prev + (1 - beta1) * g_t\n",
    "        v_t = beta2 * v_prev + (1 - beta2) * g_t ** 2\n",
    "        step = a_t * m_t / (T.sqrt(v_t + epsilon))\n",
    "\n",
    "        updates[m_prev] = m_t\n",
    "        updates[v_prev] = v_t\n",
    "        updates[param] = param - step\n",
    "\n",
    "    updates[t_prev] = t\n",
    "    return updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_mlp(input_var=None, BN=False):\n",
    "    l_in = lasagne.layers.InputLayer(shape=(None, 1, sz, sz),\n",
    "                                     input_var=input_var)\n",
    "    if BN:\n",
    "        l_in = lasagne.layers.batch_norm(l_in)\n",
    "\n",
    "    l_hid1 = lasagne.layers.DenseLayer(\n",
    "            l_in, num_units=100,\n",
    "            nonlinearity=lasagne.nonlinearities.rectify,\n",
    "            W=lasagne.init.GlorotUniform())\n",
    "    if BN:\n",
    "        l_hid1 = lasagne.layers.batch_norm(l_hid1)\n",
    "\n",
    "    l_hid2 = lasagne.layers.DenseLayer(\n",
    "            l_hid1, num_units=100,\n",
    "            nonlinearity=lasagne.nonlinearities.rectify)\n",
    "    if BN:\n",
    "        l_hid2 = lasagne.layers.batch_norm(l_hid2)\n",
    "        \n",
    "    l_hid3 = lasagne.layers.DenseLayer(\n",
    "            l_hid2, num_units=100,\n",
    "            nonlinearity=lasagne.nonlinearities.rectify)\n",
    "    if BN:\n",
    "        l_hid3 = lasagne.layers.batch_norm(l_hid3)\n",
    "\n",
    "    l_out = lasagne.layers.DenseLayer(\n",
    "            l_hid3, num_units=10,\n",
    "            nonlinearity=lasagne.nonlinearities.softmax)\n",
    "    return l_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    subprocess.call(['th', '-l', 'my_example'])\n",
    "    n, d = 10000, 3600\n",
    "    train_images = np.zeros((n, d), dtype=np.uint8)\n",
    "    train_labels = np.zeros(n, dtype=np.uint8)\n",
    "    for i in range(n):\n",
    "        image_open = Image.open(\"clMNIST/example\" + str(i) + \".png\")\n",
    "        a = np.array(image_open.getdata())\n",
    "        train_images[i] = a \n",
    "        train_labels[i] = np.uint(np.loadtxt(\"clMNIST/y\" + str(i)))\n",
    "    return np.reshape(train_images, (-1, 1, 60, 60)), np.ravel(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.arange(len(inputs))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt], targets[excerpt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sz = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_method(method, model='mlp', BN=False, num_epochs=50, alpha=0.1, mu=0.9, beta1=0.9, beta2=0.999, echo=False, \n",
    "               batch_size=100, epsilon=1e-8):\n",
    "    input_var = T.tensor4('inputs')\n",
    "    target_var = T.ivector('targets')\n",
    "\n",
    "    if echo:\n",
    "        print(\"Building model and compiling functions...\")\n",
    "    if model == 'mlp':\n",
    "        network = build_mlp(input_var, BN)\n",
    "    else:\n",
    "        print(\"Unrecognized model type %r.\" % model)\n",
    "        return\n",
    "\n",
    "    prediction = lasagne.layers.get_output(network)\n",
    "    loss = lasagne.objectives.categorical_crossentropy(prediction, target_var)\n",
    "    loss = loss.mean()\n",
    "    train_acc = T.mean(T.eq(T.argmax(prediction, axis=1), target_var),\n",
    "                 dtype=theano.config.floatX)\n",
    "\n",
    "    params = lasagne.layers.get_all_params(network, trainable=True)\n",
    "\n",
    "    if method == lasagne.updates.sgd:\n",
    "        updates = method(loss, params, learning_rate=alpha)\n",
    "    elif method == lasagne.updates.momentum:\n",
    "        updates = method(loss, params, learning_rate=alpha, momentum=mu)\n",
    "    elif method == lasagne.updates.adam:\n",
    "        updates = method(loss, params, learning_rate=alpha, beta1=beta1)\n",
    "    elif method == adam_update or method == adam_update2:\n",
    "        updates = method(loss, params, learning_rate=alpha, beta1=beta1, beta2=beta2, epsilon=epsilon)\n",
    "    else:\n",
    "        updates = method(loss, params, learning_rate=alpha, epsilon=epsilon)\n",
    "\n",
    "\n",
    "    test_prediction = lasagne.layers.get_output(network, deterministic=True)\n",
    "    test_loss = lasagne.objectives.categorical_crossentropy(test_prediction,\n",
    "                                                            target_var)\n",
    "    test_loss = test_loss.mean()\n",
    "    test_acc = T.mean(T.eq(T.argmax(test_prediction, axis=1), target_var),\n",
    "                      dtype=theano.config.floatX)\n",
    "\n",
    "    train_fn = theano.function([input_var, target_var], loss, updates=updates)\n",
    "    train_fn_acc = theano.function([input_var, target_var], train_acc)\n",
    "    val_fn = theano.function([input_var, target_var], [test_loss, test_acc])\n",
    "\n",
    "    if echo:\n",
    "        print(\"Starting training...\")\n",
    "\n",
    "    res = dict()\n",
    "    arr_train_err = []\n",
    "    arr_val_err = []\n",
    "    arr_train_acc = []\n",
    "    arr_val_acc = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_err = 0\n",
    "        train_batches = 0\n",
    "        train_acc = 0\n",
    "        start_time = time.time()\n",
    "        \n",
    "        train_inputs, train_targets = get_data()\n",
    "        \n",
    "        for batch in iterate_minibatches(train_inputs, train_targets, batch_size, shuffle=True):\n",
    "            inputs, targets = batch\n",
    "            err = train_fn(inputs, targets)\n",
    "            acc = train_fn_acc(inputs, targets)\n",
    "            train_err += err\n",
    "            train_acc += acc\n",
    "            train_batches += 1\n",
    "\n",
    "        arr_train_err.append(train_err / train_batches)\n",
    "        arr_train_acc.append(train_acc / train_batches * 100)\n",
    "        \n",
    "        val_inputs, val_targets = get_data()\n",
    "\n",
    "        val_err = 0\n",
    "        val_acc = 0\n",
    "        val_batches = 0\n",
    "        for batch in iterate_minibatches(val_inputs, val_targets, batch_size, shuffle=False):\n",
    "            inputs, targets = batch\n",
    "            err, acc = val_fn(inputs, targets)\n",
    "            val_err += err\n",
    "            val_acc += acc\n",
    "            val_batches += 1\n",
    "\n",
    "        arr_val_err.append(val_err / val_batches)\n",
    "        arr_val_acc.append(val_acc / val_batches * 100)\n",
    "\n",
    "        if echo:\n",
    "            print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "                epoch + 1, num_epochs, time.time() - start_time))\n",
    "            print(\"  training loss:\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "#             print(\"  validation loss:\\t\\t{:.6f}\".format(val_err / val_batches))\n",
    "#             print(\"  validation accuracy:\\t\\t{:.2f} %\".format(\n",
    "#                 val_acc / val_batches * 100))\n",
    "\n",
    "    test_err = 0\n",
    "    test_acc = 0\n",
    "    test_batches = 0\n",
    "    \n",
    "    test_inputs, test_targets = get_data()\n",
    "        \n",
    "    for batch in iterate_minibatches(test_inputs, test_targets, batch_size, shuffle=False):\n",
    "        inputs, targets = batch\n",
    "        err, acc = val_fn(inputs, targets)\n",
    "        test_err += err\n",
    "        test_acc += acc\n",
    "        test_batches += 1\n",
    "\n",
    "    if echo:\n",
    "        print(\"Final results:\")\n",
    "        print(\"  test loss:\\t\\t\\t{:.6f}\".format(test_err / test_batches))\n",
    "        print(\"  test accuracy:\\t\\t{:.2f} %\".format(\n",
    "            test_acc / test_batches * 100))\n",
    "\n",
    "    res['train_err'] = np.array(arr_train_err)\n",
    "    res['val_err'] = np.array(arr_val_err)\n",
    "    res['train_acc'] = np.array(arr_train_acc)\n",
    "    res['val_acc'] = np.array(arr_val_acc)\n",
    "    res['test_err'] = test_err / test_batches\n",
    "    res['test_acc'] = test_acc / test_batches * 100\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "make_seed(100)\n",
    "adam1 = run_method(adam_update, num_epochs=50, alpha=1e-3, beta2=0.999, epsilon=1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "make_seed(100)\n",
    "bn_adam1 = run_method(adam_update, num_epochs=50, alpha=1e-3, beta2=0.999, epsilon=1e-8, BN=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "make_seed(100)\n",
    "adam2 = run_method(adam_update, num_epochs=50, alpha=1e-3, beta2=0.999, epsilon=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "make_seed(100)\n",
    "bn_adam2 = run_method(adam_update, num_epochs=50, alpha=1e-3, beta2=0.999, epsilon=1e-4, BN=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "make_seed(100)\n",
    "adam3 = run_method(adam_update2, num_epochs=50, alpha=1e-3, beta2=0.999, epsilon=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "make_seed(100)\n",
    "bn_adam3 = run_method(adam_update, num_epochs=50, alpha=1e-3, beta2=0.999, epsilon=1e-6, BN=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
